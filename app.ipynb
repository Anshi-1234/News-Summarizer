{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_FILE = 'api_key.txt'\n",
    "\n",
    "with open(API_FILE, 'r') as api_content:\n",
    "    api_key = api_content.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the Latest News Articles\n",
    "We will use **NewsAPI** to retrieve news articles. You can sign up for a free API key at https://newsapi.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Meta hides warning labels for AI-edited images\n",
      "Description: Starting next week, Meta will no longer put an easy-to-see label on Facebook images that were edited using AI tools, and it will make it much harder to determine if they appear in their original state or had been doctored. To be clear, the company will still …\n",
      "Link: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_2f675784-4666-497b-8867-104365facbab\n",
      "\n",
      "Title: ByteDance will reportedly use Huawei chips to train a new AI model\n",
      "Description: As first reported by Reuters, ByteDance, the Chinese parent company of TikTok, is planning to train and develop an AI model\n",
      " using chips from fellow Chinese company Huawei. Three anonymous sources approached Reuters with this information; a fourth source cou…\n",
      "Link: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_696f4cdb-5436-43e1-b3cd-6cdaee93cbe7\n",
      "\n",
      "Title: Google is using AI to make fake podcasts from your research\n",
      "Description: Google’s AI note-taking app, NotebookLM, will now let you generate a conversation between two AI “hosts” about your research.\n",
      "Link: https://www.theverge.com/2024/9/11/24242138/google-notebook-llm-ai-fake-podcasts-research\n",
      "\n",
      "Title: Generative AI Hype Feels Inescapable. Tackle It Head On With Education\n",
      "Description: In their book AI Snake Oil, two Princeton researchers pinpoint the culprits of the AI hype cycle and advocate for a more critical, holistic understanding of artificial intelligence.\n",
      "Link: https://www.wired.com/story/artificial-intelligence-hype-ai-snake-oil/\n",
      "\n",
      "Title: Slack AI will generate transcripts and notes from huddles\n",
      "Description: Salesforce has rolled out some new AI features for its business-focused Slack chat app designed to take over mundane chores like transcription. \n",
      "A key new feature is Slack AI huddle notes to \"capture key takeaways and action items so users can focus on the wo…\n",
      "Link: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_c3de1f4c-927a-4c53-b950-00dac2a1ebe6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_news(api_key, query=\"technology\", num_articles=5):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&pageSize={num_articles}&apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    articles = response.json()[\"articles\"]\n",
    "    return [(article['title'], article['description'], article['url']) for article in articles]\n",
    "\n",
    "# Example usage\n",
    "news_articles = fetch_news(api_key, query=\"AI\", num_articles=5)\n",
    "for title, desc, url in news_articles:\n",
    "    print(f\"Title: {title}\\nDescription: {desc}\\nLink: {url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the Articles using an LLM\n",
    "Using Hugging Face's pre-trained models like **T5** for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize_articles(articles):\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "    summaries = []\n",
    "    for title, desc, url in articles:\n",
    "        full_text = f\"{title}. {desc}\"\n",
    "        summary = summarizer(full_text, max_length=50, min_length=25, do_sample=False)[0][\"summary_text\"]\n",
    "        summaries.append((title, summary, url))\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sentiment Analysis on the Articles\n",
    "We'll use **VADER** from **NLTK** for sentiment analysis. It's easy to set up and provides accurate results for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ratch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def analyze_sentiment(articles):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = []\n",
    "    for title, summary, url in articles:\n",
    "        sentiment = sia.polarity_scores(summary)[\"compound\"]\n",
    "        sentiment_label = \"Positive\" if sentiment > 0 else \"Negative\" if sentiment < 0 else \"Neutral\"\n",
    "        sentiment_scores.append((title, summary, sentiment_label, url))\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Retrieval System using FAISS\n",
    "We can implement a simple retrieval system that allows the user to input a search term and retrieve the most relevant news articles.\n",
    "**FAISS** is used to efficiently search through the dataset of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_faiss_index(articles):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    article_embeddings = model.encode([summary for _, summary, _ in articles])\n",
    "    index = faiss.IndexFlatL2(article_embeddings.shape[1])  # L2 distance for similarity\n",
    "    index.add(article_embeddings)\n",
    "    return index, article_embeddings\n",
    "\n",
    "def search_news(index, article_embeddings, query, articles):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    D, I = index.search([query_embedding], k=3)  # Retrieve top 3 articles\n",
    "    return [articles[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Frontend with Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "def display_app():\n",
    "    st.title(\"News Summarizer and Sentiment Analyzer\")\n",
    "    api_key = st.text_input(\"Enter NewsAPI Key:\")\n",
    "    query = st.text_input(\"Enter a search term (e.g., AI, Technology):\")\n",
    "    \n",
    "    if api_key and query:\n",
    "        news_articles = fetch_news(api_key, query=query, num_articles=5)\n",
    "        st.write(\"### Retrieved News Articles\")\n",
    "        for title, desc, url in news_articles:\n",
    "            st.write(f\"**{title}**\\n{desc}\\n[Read More]({url})\")\n",
    "        \n",
    "        if st.button(\"Summarize and Analyze\"):\n",
    "            summarized_articles = summarize_articles(news_articles)\n",
    "            sentiment_scores = analyze_sentiment(summarized_articles)\n",
    "            \n",
    "            st.write(\"### Summarized Articles with Sentiment\")\n",
    "            for title, summary, sentiment, url in sentiment_scores:\n",
    "                st.write(f\"**Title**: {title}\")\n",
    "                st.write(f\"**Summary**: {summary}\")\n",
    "                st.write(f\"**Sentiment**: {sentiment}\")\n",
    "                st.write(f\"[Read Full Article]({url})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Application through Terminal\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
